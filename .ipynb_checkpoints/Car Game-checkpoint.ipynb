{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import math as m\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x ,y = 90,51\n",
    "vel = 0\n",
    "rad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(p1,p2,p3,p4):\n",
    "    rhs = np.array(p4 - p2)\n",
    "    lhs = np.array([ p1 - p2, p4 - p3 ])\n",
    "    lhs = np.transpose(lhs)\n",
    "    try:\n",
    "        res = np.linalg.solve(lhs,rhs)\n",
    "    except:\n",
    "        res = [ float('inf'), float('inf') ]\n",
    "    return res\n",
    "\n",
    "\n",
    "def distancelineshape( p1, p2, shape ):\n",
    "    for i in range(len(shape)):\n",
    "        nexti  = ( i + 1 ) % len(shape)\n",
    "        alphas = intersection(p1,p2,shape[i],shape[nexti])\n",
    "        if(alphas[0]==float('inf') or alphas[0]>1):\n",
    "            return float('inf')\n",
    "        else:\n",
    "            return np.linalg.norm(p1-p2)*(1-alphas[0])\n",
    "    return float('inf')\n",
    "    \n",
    "def detect_coll(shape1, shape2):\n",
    "    for i in range(len(shape1)):\n",
    "        nexti = ( i + 1 ) % len(shape1)\n",
    "        for j in range(len(shape2)):            \n",
    "            nextj   = ( j + 1 ) % len(shape2)\n",
    "            alphas  = intersection(shape1[i],shape1[nexti],shape2[j],shape2[nextj])\n",
    "            if( max(alphas) <= 1 and min(alphas) >= 0 ):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_actions, numInp):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, numInp])\n",
    "        \n",
    "        \n",
    "        hidden_layer = tf.layers.dense(self.input_layer, 12, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_2 = tf.layers.dense(hidden_layer, 12, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_3 = tf.layers.dense(hidden_layer_2, 12, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_4 = tf.layers.dense(hidden_layer_3, 12, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        out = tf.layers.dense(hidden_layer_4, num_actions, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.softmax(out)\n",
    "        self.choice = tf.argmax(self.outputs, axis=1)\n",
    "        \n",
    "        self.rewards = tf.placeholder(shape=[None, ], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        one_hot_actions = tf.one_hot(self.actions, num_actions)\n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=one_hot_actions)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(cross_entropy * self.rewards)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tf.trainable_variables())\n",
    "        \n",
    "        self.gradients_to_apply = []\n",
    "        for index, variable in enumerate(tf.trainable_variables()):\n",
    "            gradient_placeholder = tf.placeholder(tf.float32)\n",
    "            self.gradients_to_apply.append(gradient_placeholder)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        self.update_gradients = optimizer.apply_gradients(zip(self.gradients_to_apply, tf.trainable_variables()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_game(control):\n",
    "    done = False\n",
    "    pygame.time.delay(0)\n",
    "    global x \n",
    "    global y\n",
    "    global rad\n",
    "    global vel\n",
    "    global rewards\n",
    "    global track1\n",
    "    global track2\n",
    "    global reward_added\n",
    "    reward = 0    \n",
    "\n",
    "    points = []\n",
    "    points.append([x + m.cos(rad) * 10, y + m.sin(rad)*10])\n",
    "    points.append([x + m.cos(rad + m.pi * 6/7) * 10, y + m.sin(rad + m.pi*6/7)*10])\n",
    "    points.append([x + m.cos(rad + m.pi*8/7) * 10, y + m.sin(rad+m.pi*8/7)*10])\n",
    "    points = np.array(points)\n",
    "    \n",
    "    for event in pygame.event.get():\n",
    "        if(event.type == pygame.QUIT):\n",
    "            run = False\n",
    "    keys= pygame.key.get_pressed()\n",
    "    if(control == 0):\n",
    "        rad -= 0.2\n",
    "    if(control == 2):\n",
    "        rad +=0.2\n",
    "    if(control == 1):\n",
    "        vel +=2\n",
    "    if(keys[pygame.K_q]):\n",
    "        pygame.quit()\n",
    "    x +=m.cos(rad) * vel\n",
    "    y +=m.sin(rad) * vel\n",
    "    vel *=0.85\n",
    "    win.fill((255,255,255))\n",
    "    \n",
    "    pygame.draw.polygon(win, (255,0,0), points)\n",
    "    if(detect_coll(points, track1) or detect_coll(points, track2)):\n",
    "        x ,y = 90,50\n",
    "        vel = 0\n",
    "        rad = 0\n",
    "        done = True\n",
    "    i = 0\n",
    "    while (i<len(rewards)):\n",
    "        if(detect_coll(points, np.array(rewards[i]))):\n",
    "            reward += reward_added\n",
    "            reward_added +=100\n",
    "            del rewards[i]\n",
    "            i-=1\n",
    "        i+=1\n",
    "            \n",
    "    \n",
    "    distances = []\n",
    "    \n",
    "    num_dist = 8\n",
    "    for i in range (num_dist):\n",
    "        dist1= distancelineshape(np.array([x,y]),np.array([x + 1000*m.cos(rad + m.pi*i/num_dist * 2), y + 1000*m.sin(rad+m.pi*i/num_dist * 2)]),track1)\n",
    "        dist2= distancelineshape(np.array([x,y]),np.array([x + 1000*m.cos(rad + m.pi*i/num_dist * 2), y + 1000*m.sin(rad+m.pi*i/num_dist * 2)]),track2)\n",
    "        distances.append(min(dist1,dist2))\n",
    "        \n",
    "    pygame.draw.lines(win, (100,100,100), True,track1 ,1)\n",
    "    pygame.draw.lines(win, (100,100,100), True,track2 ,1)\n",
    "    pygame.display.update()\n",
    "    return distances, reward, done\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_rate = 0.95\n",
    "def discount_normalize_rewards(rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        total_rewards = total_rewards*discount_rate + rewards[i]\n",
    "        discounted_rewards[i] = total_rewards\n",
    "    \n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= np.std(discounted_rewards)\n",
    "    return(discounted_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-d2121a3fa44a>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-d2121a3fa44a>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Average reward / 100 eps: 500.0\n",
      "Average reward / 100 eps: 268.0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Average reward / 100 eps: 246.0\n",
      "Average reward / 100 eps: 284.0\n",
      "Average reward / 100 eps: 299.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5dc9b785231c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_gradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[0mgradient_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO Create the training loop\n",
    "tf.reset_default_graph()\n",
    "num_actions = 3\n",
    "state_size = 8\n",
    "path = \"./CarGame-pg/\"\n",
    "\n",
    "training_episodes = 1000\n",
    "max_steps_per_episode = 1000\n",
    "episode_batch_size = 100\n",
    "\n",
    "agent = Agent(num_actions,state_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_episode_rewards = []\n",
    "    gradient_buffer = sess.run(tf.trainable_variables())\n",
    "    for index, gradient in enumerate(gradient_buffer):\n",
    "        gradient_buffer[index] = gradient * 0\n",
    "    episode = 0\n",
    "    while True:\n",
    "        pygame.init()\n",
    "        track1 = np.array([[70, 380],[70, 325],[70, 270],[80, 165],[65, 132],[50, 100],[50,60],[50, 20],[240, 20],[280, 100],[320, 180],[320, 220],[320, 260],[280, 380],[70, 380]])\n",
    "        track2 = np.array([[120, 330],[120,300],[120, 270],[130, 165],[115, 132],[100, 100],[100, 85],[100,  70],[145, 70],[190,  70],[230, 125],[270, 180],[270, 260],[230, 330],[120, 330]])\n",
    "        rewards = list(map(lambda i, j:[list(i),list(j)], track1, track2))\n",
    "        reward_added = 400\n",
    "        win = pygame.display.set_mode((500,500))\n",
    "        pygame.display.set_caption(\"Car Game\")\n",
    "        x = 90\n",
    "        y = 51\n",
    "        rad = 0\n",
    "        vel = 0\n",
    "        distances,reward,done = run_game(-1)\n",
    "        \n",
    "        episode_history = []\n",
    "        episode_rewards = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "            \n",
    "           \n",
    "            \n",
    "            action_probabilities = sess.run(agent.outputs, feed_dict={agent.input_layer: [distances]})\n",
    "            action_choice = np.random.choice(range(num_actions), p=action_probabilities[0])\n",
    "            distances_new, reward, done = run_game(action_choice)\n",
    "#            print(distances_new)\n",
    "            episode_history.append([distances, action_choice, reward, distances_new])\n",
    "            distances = distances_new\n",
    "            episode_rewards += reward\n",
    "            \n",
    "            if done or step + 1 == max_steps_per_episode:\n",
    "                total_episode_rewards.append(episode_rewards)\n",
    "                episode_history = np.array(episode_history)\n",
    "                ep_gradients = sess.run(agent.gradients, feed_dict={agent.input_layer: np.vstack(episode_history[:, 0]),\n",
    "                                                                    agent.actions: episode_history[:, 1],\n",
    "                                                                    agent.rewards: episode_history[:, 2]})\n",
    "                # add the gradients to the grad buffer:\n",
    "                for index, gradient in enumerate(ep_gradients):\n",
    "                    gradient_buffer[index] += gradient\n",
    "                pygame.quit()\n",
    "                break\n",
    "            \n",
    "        if episode % episode_batch_size == 0:\n",
    "        \n",
    "            feed_dict_gradients = dict(zip(agent.gradients_to_apply, gradient_buffer))\n",
    "            \n",
    "            sess.run(agent.update_gradients, feed_dict=feed_dict_gradients)\n",
    "            \n",
    "            for index, gradient in enumerate(gradient_buffer):\n",
    "                gradient_buffer[index] = gradient * 0\n",
    "                \n",
    "            if episode % 1000 == 0:\n",
    "                saver.save(sess, path + \"pg-checkpoint\", episode)\n",
    "                print(\"Average reward / 100 eps: \" + str(np.mean(total_episode_rewards[-100:])))\n",
    "        episode+=1\n",
    "    print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "num_dist = 8\n",
    "distancelineshape(np.array([x,y]),np.array([x + m.cos(rad + m.pi*i/num_dist * 2), y + m.sin(rad+m.pi*i/num_dist * 2)]),track2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
