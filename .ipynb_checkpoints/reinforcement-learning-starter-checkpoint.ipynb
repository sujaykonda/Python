{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning with OpenAI Gym\n",
    "---\n",
    "This notebook will create and test different reinforcement learning agents and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Environment\n",
    "---\n",
    "Call `gym.make(\"environment name\")` to load a new environment.\n",
    "\n",
    "Check out the list of available environments at <https://gym.openai.com/envs/>\n",
    "\n",
    "Edit this cell to load different environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idstudent/.conda/envs/tensorflow-env/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load an environment\n",
    "env = gym.make(\"CartPole-v1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print observation and action spaces\n",
    "print (env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an Agent\n",
    "---\n",
    "\n",
    "Reset the environment before each run with `env.reset`\n",
    "\n",
    "Step forward through the environment to get new observations and rewards over time with `env.step`\n",
    "\n",
    "`env.step` takes a parameter for the action to take on this step and returns the following:\n",
    "- Observations for this step\n",
    "- Rewards earned this step\n",
    "- \"Done\", a boolean value indicating if the game is finished\n",
    "- Info - some debug information that some environments provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "21.0\n",
      "15.0\n",
      "13.0\n",
      "17.0\n",
      "21.0\n",
      "15.0\n",
      "19.0\n",
      "18.0\n",
      "35.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Make a random agent\n",
    "games_to_play = 10\n",
    "for i in range(games_to_play):\n",
    "    obs=env.reset()\n",
    "    episode_rewards = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "    print(episode_rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradients\n",
    "---\n",
    "The policy gradients algorithm records gameplay over a training period, then runs the results of the actions chosen through a neural network, making successful actions that resulted in a reward more likely, and unsuccessful actions less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build the policy gradient neural network\n",
    "class Agent:\n",
    "    def __init__(self, num_actions, state_size):\n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        self.input_layer = tf.placeholder(dtype = tf.float32, shape=[None, state_size])\n",
    "        hidden_layer = tf.layers.dense(self.input_layer, 8, activation=tf.nn.relu, \n",
    "                                        kernel_initializer=initializer)\n",
    "        hidden_layer_2 = tf.layers.dense(self.input_layer, 8, activation=tf.nn.relu, \n",
    "                                        kernel_initializer=initializer)\n",
    "        out = tf.layers.dense(hidden_layer_2, num_actions, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.softmax(out)\n",
    "        self.choice = tf.argmax(self.outputs,axis = 1)\n",
    "        \n",
    "        self.rewards = tf.placeholder(shape = [None,], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None,],dtype=tf.int32)\n",
    "        \n",
    "        one_hot_actions = tf.one_hot(self.actions, num_actions)\n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = out, \n",
    "                                                                labels = one_hot_actions)\n",
    "        self.loss = tf.reduce_mean(cross_entropy * self.rewards)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tf.trainable_variables())\n",
    "        self.gradients_to_apply = []\n",
    "        for index, variable in enumerate(tf.trainable_variables()):\n",
    "            gradient_placeholder = tf.placeholder(tf.float32)\n",
    "            self.gradients_to_apply.append(gradient_placeholder)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "        self.update_gradients = optimizer.apply_gradients(zip(self.gradients_to_apply,\n",
    "                                                              tf.trainable_variables()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounting and Normalizing Rewards\n",
    "---\n",
    "In order to determine how \"successful\" a given action is, the policy gradient algorithm evaluates each action based on how many rewards were earned after it was performed in an episode.\n",
    "\n",
    "The discount rewards function goes through each time step of an episode and tracks the total rewards earned from each step to the end of the episode.\n",
    "\n",
    "For example, if an episode took 10 steps to finish, and the agent earns 1 point of reward every step, the rewards for each frame would be stored as \n",
    "`[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]`\n",
    "\n",
    "This allows the agent to credit early actions that didn't lose the game with future success, and later actions (that likely resulted in the end of the game) to get less credit.\n",
    "\n",
    "One disadvantage of arranging rewards like this is that early actions didn't necessarily directly contribute to later rewards, so a **discount factor** is applied that scales rewards down over time. A discount factor < 1 means that rewards earned closer to the current time step will be worth more than rewards earned later.\n",
    "\n",
    "With our reward example above, if we applied a discount factor of .90, the rewards would be stored as\n",
    "`[ 6.5132156   6.12579511  5.6953279   5.217031    4.68559     4.0951      3.439\n",
    "  2.71        1.9         1. ]`\n",
    "\n",
    "This means that the early actions still get more credit than later actions, but not the full value of the rewards for the entire episode.\n",
    "\n",
    "Finally, the rewards are normalized to lower the variance between reward values in longer or shorter episodes.\n",
    "\n",
    "You can tweak the discount factor as one of the hyperparameters of your model to find one that fits your task the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the discounted and normalized rewards function\n",
    "discount_rate = 0.95\n",
    "def discount_normalize_rewards(rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        total_rewards = total_rewards*discount_rate + rewards[i]\n",
    "        discounted_rewards[i] = total_rewards\n",
    "    \n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= np.std(discounted_rewards)\n",
    "    return(discounted_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure\n",
    "---\n",
    "The agent will play games and record the history of the episode. At the end of every game, the episode's history will be processed to calculate the **gradients** that the model learned from that episode.\n",
    "\n",
    "Every few games the calculated gradients will be applied, updating the model's parameters with the lessons from the games so far.\n",
    "\n",
    "While training, you'll keep track of average scores and render the environment occasionally to see your model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-cc0eab3dc4d4>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-cc0eab3dc4d4>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    ep_gradients = sess.run(agent.gradients, feed_dict={agent.input_layer: np.vstack(episode_history[:,0]), 0]),\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# TODO Create the training loop\n",
    "tf.reset_default_graph()\n",
    "num_actions = 2\n",
    "state_size = 4\n",
    "path = \"./cartpole-pg/\"\n",
    "training_episodes = 1000\n",
    "max_steps_per_episode = 10000\n",
    "episode_batch_size = 5\n",
    "agent = Agent(num_actions, state_size)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep = 2)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_episode_rewards = []\n",
    "    gradient_buffer=sess.run(tf.trainable_variables())\n",
    "    for index, gradient in enumerate (gradient_buffer):\n",
    "        gradient_buffer[index] = gradient*0\n",
    "    for episode in range(training_episodes):\n",
    "        state = env.reset()\n",
    "        episode_history = []\n",
    "        episode_reward = 0\n",
    "        for step in range(max_steps_per_episode):\n",
    "            if episode%100 == 0:\n",
    "                env.render()\n",
    "            action_probabilities = sess.run(agent.outputs, feed_dict={agent.input_layer: [state]}) \n",
    "            action_choice = np.random.choice(range(num_actions), p = action_probabilities[0])\n",
    "            state_next, reward,done,_ = env.step(action_choice)\n",
    "            episode_history.append([state, action_choice, reward, state_next])\n",
    "            state = state_next\n",
    "            episode_reward += reward\n",
    "            if done or step + 1 == max_steps_per_episode: \n",
    "                episode_history = np.array(episode_history) \n",
    "                episode_history[:,2] = discount_normalize_rewards(episode_history[:,2])\n",
    "                ep_gradients = sess.run(agent.gradients, feed_dict={agent.input_layer: np.vstack(episode_history[:,0]), 0]), \n",
    "                agent.actions: episode_history[:, 1], agent.rewards: episode_history[:, 2]}) \n",
    "                for index, gradient in enumerate(ep_gradients):\n",
    "                    gradient_buffer[index] += gradient\n",
    "                    break\n",
    "            if episode % episode_batch_size == 0:\n",
    "        \n",
    "                feed_dict_gradients = dict(zip(agent.gradients_to_apply, gradient_buffer))\n",
    "            \n",
    "                sess.run(agent.update_gradients, feed_dict=feed_dict_gradients)\n",
    "            \n",
    "            for index, gradient in enumerate(gradient_buffer):\n",
    "                gradient_buffer[index] = gradient * 0\n",
    "                \n",
    "            if episode % 100 == 0:\n",
    "                saver.save(sess, path + \"pg-checkpoint\", episode)\n",
    "                print(\"Average reward / 100 eps: \" + str(np.mean(total_episode_rewards[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "---\n",
    "\n",
    "This cell will run through games choosing actions without the learning process so you can see how your model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to close the environment\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
